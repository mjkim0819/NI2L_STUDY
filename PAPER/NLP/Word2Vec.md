# Word2Vec
결과값이 출력층으로만 향하지 않고, 출력층과 동시에 다시 은닉층의 다음 계산 입력으로 들어감

## Word Embedding
![image](https://github.com/mjkim0819/NI2L_STUDY/assets/108729047/237035bf-6018-4bf5-8c83-b5d982ea1034)  
왼쪽은 희소 표현, 오른쪽은 밀집 표현  
  
### 희소 표현(Sparse Representation)
벡터 또는 행렬(matrix)의 대부분 값이 0으로 표현되는 방법  
Ex) 원 핫 벡터 [ 0, 0, 0, 0, 1, 0, 0 ]  
    
- 장점
  - 표현하고자 하는 단어를 간단하게 표현
- 단점
  - 데이터셋 내 단어가 많으면, 벡터가 기하급수적으로 커짐
  - 원 핫 벡터처럼 단위 벡터로 표현하면 단어간 유사 관계를 나타낼 수 없음  

  
  
### 밀집 표현(Dense Representation)
모든 단어의 벡터 표현 차원을 맞춰 각 벡터 값이 0과 1이 아닌 실수 값으로 표현되는 방법  
Ex) 워드 임베딩 (단어를 밀집 벡터 형태로 표현)  
    
- 장점
  - 저차원에서 단어의 의미를 여러 차원의 공간에 분산하여 표현하기 때문에 단어의 유사도 표현 가능 
  
### 희소 표현과 밀집 표현의 비교
데이터가 10000개 있을 때  
- 희소표현
  - 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이때 1 뒤의 0의 수는 9995개.  -> 차원은 10000
- 밀집표현
  - 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128  
![image](https://github.com/mjkim0819/NI2L_STUDY/assets/108729047/037005fb-aeae-4d4a-9146-e4123a2cfbb9)    
  
  
## Word2Vec
**분포 가설(distributional hypothesis)** 을 가정 하에 표현한 분산 표현을 따라 **벡터값을 통해 의미적으로 가까운 단어** 를 찾는 방법  
ex)  ‘강아지’라는 단어는 ‘귀엽다’, ‘예쁘다’, ‘애교’ 등의 단어와 자주 사용된다면, 단어들의 벡터화 값이 유사할 것이다.  
이미 분포 가설을 사용한 경우는 많지만 Word2Vec는 **효율성** 문제로 주목받게 됨.  

아래는 Word2Vec의 대표적인 학습 방법 2가지  
### CBoW(Continuous Bag of Words)
주변에 있는 단어들로 중간에 있는 단어들을 예측하는 방법  
#### 용어 설명
### Skip-Gram

## Word2Vec의 최적화
### Hierarchical softmax
### Negative sampling
### 성능평가

## 한계점
