{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor"
      ],
      "metadata": {
        "id": "3t_Qy-sMgnoM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])"
      ],
      "metadata": {
        "id": "1Tuna3iAgq6N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "9DnzMTmFgtzR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "rCDjU2zOg5l_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wovZEJz3geuw",
        "outputId": "cfeddfe8-295c-4bd3-ba39-2ec9ec4c3aeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 41.816070556640625 \n",
            "Epoch: 1 | Loss: 18.672588348388672 \n",
            "Epoch: 2 | Loss: 8.368949890136719 \n",
            "Epoch: 3 | Loss: 3.7812509536743164 \n",
            "Epoch: 4 | Loss: 1.7381372451782227 \n",
            "Epoch: 5 | Loss: 0.827812910079956 \n",
            "Epoch: 6 | Loss: 0.4217860698699951 \n",
            "Epoch: 7 | Loss: 0.24026867747306824 \n",
            "Epoch: 8 | Loss: 0.1587074249982834 \n",
            "Epoch: 9 | Loss: 0.12165544182062149 \n",
            "Epoch: 10 | Loss: 0.10442793369293213 \n",
            "Epoch: 11 | Loss: 0.09603619575500488 \n",
            "Epoch: 12 | Loss: 0.0915885716676712 \n",
            "Epoch: 13 | Loss: 0.08890664577484131 \n",
            "Epoch: 14 | Loss: 0.08702097833156586 \n",
            "Epoch: 15 | Loss: 0.08549961447715759 \n",
            "Epoch: 16 | Loss: 0.08415046334266663 \n",
            "Epoch: 17 | Loss: 0.0828874409198761 \n",
            "Epoch: 18 | Loss: 0.08167228102684021 \n",
            "Epoch: 19 | Loss: 0.08048787713050842 \n",
            "Epoch: 20 | Loss: 0.07932652533054352 \n",
            "Epoch: 21 | Loss: 0.07818415760993958 \n",
            "Epoch: 22 | Loss: 0.07705968618392944 \n",
            "Epoch: 23 | Loss: 0.07595182210206985 \n",
            "Epoch: 24 | Loss: 0.07486007362604141 \n",
            "Epoch: 25 | Loss: 0.07378412038087845 \n",
            "Epoch: 26 | Loss: 0.07272368669509888 \n",
            "Epoch: 27 | Loss: 0.07167846709489822 \n",
            "Epoch: 28 | Loss: 0.07064839452505112 \n",
            "Epoch: 29 | Loss: 0.06963308155536652 \n",
            "Epoch: 30 | Loss: 0.06863224506378174 \n",
            "Epoch: 31 | Loss: 0.0676460713148117 \n",
            "Epoch: 32 | Loss: 0.06667380034923553 \n",
            "Epoch: 33 | Loss: 0.06571557372808456 \n",
            "Epoch: 34 | Loss: 0.0647711306810379 \n",
            "Epoch: 35 | Loss: 0.06384024769067764 \n",
            "Epoch: 36 | Loss: 0.06292276829481125 \n",
            "Epoch: 37 | Loss: 0.06201860308647156 \n",
            "Epoch: 38 | Loss: 0.06112721934914589 \n",
            "Epoch: 39 | Loss: 0.06024865433573723 \n",
            "Epoch: 40 | Loss: 0.05938279628753662 \n",
            "Epoch: 41 | Loss: 0.05852942168712616 \n",
            "Epoch: 42 | Loss: 0.057688288390636444 \n",
            "Epoch: 43 | Loss: 0.056859150528907776 \n",
            "Epoch: 44 | Loss: 0.05604204908013344 \n",
            "Epoch: 45 | Loss: 0.05523648113012314 \n",
            "Epoch: 46 | Loss: 0.054442718625068665 \n",
            "Epoch: 47 | Loss: 0.053660258650779724 \n",
            "Epoch: 48 | Loss: 0.05288916453719139 \n",
            "Epoch: 49 | Loss: 0.05212898552417755 \n",
            "Epoch: 50 | Loss: 0.05137982591986656 \n",
            "Epoch: 51 | Loss: 0.05064137652516365 \n",
            "Epoch: 52 | Loss: 0.04991360753774643 \n",
            "Epoch: 53 | Loss: 0.049196306616067886 \n",
            "Epoch: 54 | Loss: 0.048489294946193695 \n",
            "Epoch: 55 | Loss: 0.04779241979122162 \n",
            "Epoch: 56 | Loss: 0.0471055805683136 \n",
            "Epoch: 57 | Loss: 0.04642856866121292 \n",
            "Epoch: 58 | Loss: 0.04576129466295242 \n",
            "Epoch: 59 | Loss: 0.04510365426540375 \n",
            "Epoch: 60 | Loss: 0.04445545747876167 \n",
            "Epoch: 61 | Loss: 0.043816469609737396 \n",
            "Epoch: 62 | Loss: 0.04318685084581375 \n",
            "Epoch: 63 | Loss: 0.042566243559122086 \n",
            "Epoch: 64 | Loss: 0.04195437580347061 \n",
            "Epoch: 65 | Loss: 0.04135145992040634 \n",
            "Epoch: 66 | Loss: 0.04075723513960838 \n",
            "Epoch: 67 | Loss: 0.04017135500907898 \n",
            "Epoch: 68 | Loss: 0.03959408029913902 \n",
            "Epoch: 69 | Loss: 0.039025094360113144 \n",
            "Epoch: 70 | Loss: 0.03846420347690582 \n",
            "Epoch: 71 | Loss: 0.03791147470474243 \n",
            "Epoch: 72 | Loss: 0.0373666025698185 \n",
            "Epoch: 73 | Loss: 0.03682957589626312 \n",
            "Epoch: 74 | Loss: 0.03630032390356064 \n",
            "Epoch: 75 | Loss: 0.03577852249145508 \n",
            "Epoch: 76 | Loss: 0.03526431322097778 \n",
            "Epoch: 77 | Loss: 0.0347575843334198 \n",
            "Epoch: 78 | Loss: 0.03425804153084755 \n",
            "Epoch: 79 | Loss: 0.033765748143196106 \n",
            "Epoch: 80 | Loss: 0.03328036516904831 \n",
            "Epoch: 81 | Loss: 0.032802239060401917 \n",
            "Epoch: 82 | Loss: 0.032330676913261414 \n",
            "Epoch: 83 | Loss: 0.03186611831188202 \n",
            "Epoch: 84 | Loss: 0.03140804544091225 \n",
            "Epoch: 85 | Loss: 0.030956828966736794 \n",
            "Epoch: 86 | Loss: 0.03051184117794037 \n",
            "Epoch: 87 | Loss: 0.030073381960392 \n",
            "Epoch: 88 | Loss: 0.02964107319712639 \n",
            "Epoch: 89 | Loss: 0.029215097427368164 \n",
            "Epoch: 90 | Loss: 0.028795329853892326 \n",
            "Epoch: 91 | Loss: 0.028381407260894775 \n",
            "Epoch: 92 | Loss: 0.027973506599664688 \n",
            "Epoch: 93 | Loss: 0.027571549639105797 \n",
            "Epoch: 94 | Loss: 0.027175312861800194 \n",
            "Epoch: 95 | Loss: 0.026784710586071014 \n",
            "Epoch: 96 | Loss: 0.026399817317724228 \n",
            "Epoch: 97 | Loss: 0.02602040208876133 \n",
            "Epoch: 98 | Loss: 0.025646425783634186 \n",
            "Epoch: 99 | Loss: 0.02527785301208496 \n",
            "Epoch: 100 | Loss: 0.024914575740695 \n",
            "Epoch: 101 | Loss: 0.024556485936045647 \n",
            "Epoch: 102 | Loss: 0.024203576147556305 \n",
            "Epoch: 103 | Loss: 0.02385576255619526 \n",
            "Epoch: 104 | Loss: 0.023512933403253555 \n",
            "Epoch: 105 | Loss: 0.023174960166215897 \n",
            "Epoch: 106 | Loss: 0.022841867059469223 \n",
            "Epoch: 107 | Loss: 0.02251359447836876 \n",
            "Epoch: 108 | Loss: 0.022190049290657043 \n",
            "Epoch: 109 | Loss: 0.021871207281947136 \n",
            "Epoch: 110 | Loss: 0.021556897088885307 \n",
            "Epoch: 111 | Loss: 0.02124701626598835 \n",
            "Epoch: 112 | Loss: 0.020941710099577904 \n",
            "Epoch: 113 | Loss: 0.02064073458313942 \n",
            "Epoch: 114 | Loss: 0.020344117656350136 \n",
            "Epoch: 115 | Loss: 0.020051732659339905 \n",
            "Epoch: 116 | Loss: 0.019763529300689697 \n",
            "Epoch: 117 | Loss: 0.019479556009173393 \n",
            "Epoch: 118 | Loss: 0.019199538975954056 \n",
            "Epoch: 119 | Loss: 0.018923597410321236 \n",
            "Epoch: 120 | Loss: 0.018651677295565605 \n",
            "Epoch: 121 | Loss: 0.018383586779236794 \n",
            "Epoch: 122 | Loss: 0.01811949536204338 \n",
            "Epoch: 123 | Loss: 0.01785905286669731 \n",
            "Epoch: 124 | Loss: 0.01760237291455269 \n",
            "Epoch: 125 | Loss: 0.01734941080212593 \n",
            "Epoch: 126 | Loss: 0.01710004359483719 \n",
            "Epoch: 127 | Loss: 0.01685434579849243 \n",
            "Epoch: 128 | Loss: 0.016612021252512932 \n",
            "Epoch: 129 | Loss: 0.01637333445250988 \n",
            "Epoch: 130 | Loss: 0.016137992963194847 \n",
            "Epoch: 131 | Loss: 0.01590612903237343 \n",
            "Epoch: 132 | Loss: 0.015677450224757195 \n",
            "Epoch: 133 | Loss: 0.01545216329395771 \n",
            "Epoch: 134 | Loss: 0.015230126678943634 \n",
            "Epoch: 135 | Loss: 0.015011180192232132 \n",
            "Epoch: 136 | Loss: 0.014795500785112381 \n",
            "Epoch: 137 | Loss: 0.014582856558263302 \n",
            "Epoch: 138 | Loss: 0.014373327605426311 \n",
            "Epoch: 139 | Loss: 0.014166753739118576 \n",
            "Epoch: 140 | Loss: 0.013963105157017708 \n",
            "Epoch: 141 | Loss: 0.01376245729625225 \n",
            "Epoch: 142 | Loss: 0.0135646378621459 \n",
            "Epoch: 143 | Loss: 0.01336970180273056 \n",
            "Epoch: 144 | Loss: 0.013177577406167984 \n",
            "Epoch: 145 | Loss: 0.01298818364739418 \n",
            "Epoch: 146 | Loss: 0.01280152890831232 \n",
            "Epoch: 147 | Loss: 0.012617557309567928 \n",
            "Epoch: 148 | Loss: 0.012436220422387123 \n",
            "Epoch: 149 | Loss: 0.012257489375770092 \n",
            "Epoch: 150 | Loss: 0.012081337161362171 \n",
            "Epoch: 151 | Loss: 0.01190769113600254 \n",
            "Epoch: 152 | Loss: 0.011736536398530006 \n",
            "Epoch: 153 | Loss: 0.01156788319349289 \n",
            "Epoch: 154 | Loss: 0.011401649564504623 \n",
            "Epoch: 155 | Loss: 0.01123782154172659 \n",
            "Epoch: 156 | Loss: 0.011076316237449646 \n",
            "Epoch: 157 | Loss: 0.010917061939835548 \n",
            "Epoch: 158 | Loss: 0.01076018437743187 \n",
            "Epoch: 159 | Loss: 0.01060557458549738 \n",
            "Epoch: 160 | Loss: 0.01045314036309719 \n",
            "Epoch: 161 | Loss: 0.01030291523784399 \n",
            "Epoch: 162 | Loss: 0.010154861025512218 \n",
            "Epoch: 163 | Loss: 0.01000893022865057 \n",
            "Epoch: 164 | Loss: 0.00986504927277565 \n",
            "Epoch: 165 | Loss: 0.009723282419145107 \n",
            "Epoch: 166 | Loss: 0.009583588689565659 \n",
            "Epoch: 167 | Loss: 0.009445805102586746 \n",
            "Epoch: 168 | Loss: 0.009310034103691578 \n",
            "Epoch: 169 | Loss: 0.009176226332783699 \n",
            "Epoch: 170 | Loss: 0.009044392965734005 \n",
            "Epoch: 171 | Loss: 0.008914402686059475 \n",
            "Epoch: 172 | Loss: 0.008786268532276154 \n",
            "Epoch: 173 | Loss: 0.008659977465867996 \n",
            "Epoch: 174 | Loss: 0.008535567671060562 \n",
            "Epoch: 175 | Loss: 0.008412894792854786 \n",
            "Epoch: 176 | Loss: 0.008291983976960182 \n",
            "Epoch: 177 | Loss: 0.008172836154699326 \n",
            "Epoch: 178 | Loss: 0.008055348880589008 \n",
            "Epoch: 179 | Loss: 0.007939604111015797 \n",
            "Epoch: 180 | Loss: 0.007825483568012714 \n",
            "Epoch: 181 | Loss: 0.007713041268289089 \n",
            "Epoch: 182 | Loss: 0.0076021733693778515 \n",
            "Epoch: 183 | Loss: 0.0074929287657141685 \n",
            "Epoch: 184 | Loss: 0.007385234348475933 \n",
            "Epoch: 185 | Loss: 0.007279093377292156 \n",
            "Epoch: 186 | Loss: 0.007174478843808174 \n",
            "Epoch: 187 | Loss: 0.0070713600143790245 \n",
            "Epoch: 188 | Loss: 0.006969715468585491 \n",
            "Epoch: 189 | Loss: 0.0068695638328790665 \n",
            "Epoch: 190 | Loss: 0.006770849693566561 \n",
            "Epoch: 191 | Loss: 0.006673524156212807 \n",
            "Epoch: 192 | Loss: 0.006577652879059315 \n",
            "Epoch: 193 | Loss: 0.006483089178800583 \n",
            "Epoch: 194 | Loss: 0.006389953661710024 \n",
            "Epoch: 195 | Loss: 0.006298095919191837 \n",
            "Epoch: 196 | Loss: 0.006207563914358616 \n",
            "Epoch: 197 | Loss: 0.006118373014032841 \n",
            "Epoch: 198 | Loss: 0.006030459888279438 \n",
            "Epoch: 199 | Loss: 0.005943752825260162 \n",
            "Epoch: 200 | Loss: 0.005858371499925852 \n",
            "Epoch: 201 | Loss: 0.0057741389609873295 \n",
            "Epoch: 202 | Loss: 0.005691177677363157 \n",
            "Epoch: 203 | Loss: 0.0056093973107635975 \n",
            "Epoch: 204 | Loss: 0.005528752226382494 \n",
            "Epoch: 205 | Loss: 0.005449305288493633 \n",
            "Epoch: 206 | Loss: 0.005371013190597296 \n",
            "Epoch: 207 | Loss: 0.005293807480484247 \n",
            "Epoch: 208 | Loss: 0.005217711441218853 \n",
            "Epoch: 209 | Loss: 0.005142753478139639 \n",
            "Epoch: 210 | Loss: 0.00506883068010211 \n",
            "Epoch: 211 | Loss: 0.004995997995138168 \n",
            "Epoch: 212 | Loss: 0.004924178123474121 \n",
            "Epoch: 213 | Loss: 0.004853416234254837 \n",
            "Epoch: 214 | Loss: 0.004783653654158115 \n",
            "Epoch: 215 | Loss: 0.004714943468570709 \n",
            "Epoch: 216 | Loss: 0.0046471660025417805 \n",
            "Epoch: 217 | Loss: 0.004580346867442131 \n",
            "Epoch: 218 | Loss: 0.0045145670883357525 \n",
            "Epoch: 219 | Loss: 0.004449670203030109 \n",
            "Epoch: 220 | Loss: 0.004385732114315033 \n",
            "Epoch: 221 | Loss: 0.0043226582929492 \n",
            "Epoch: 222 | Loss: 0.004260541871190071 \n",
            "Epoch: 223 | Loss: 0.004199342802166939 \n",
            "Epoch: 224 | Loss: 0.004138963297009468 \n",
            "Epoch: 225 | Loss: 0.004079500678926706 \n",
            "Epoch: 226 | Loss: 0.004020881839096546 \n",
            "Epoch: 227 | Loss: 0.00396308908239007 \n",
            "Epoch: 228 | Loss: 0.003906132420524955 \n",
            "Epoch: 229 | Loss: 0.0038500071968883276 \n",
            "Epoch: 230 | Loss: 0.0037946421653032303 \n",
            "Epoch: 231 | Loss: 0.0037401122972369194 \n",
            "Epoch: 232 | Loss: 0.003686358220875263 \n",
            "Epoch: 233 | Loss: 0.0036333962343633175 \n",
            "Epoch: 234 | Loss: 0.003581179305911064 \n",
            "Epoch: 235 | Loss: 0.003529696259647608 \n",
            "Epoch: 236 | Loss: 0.0034789731726050377 \n",
            "Epoch: 237 | Loss: 0.003428978379815817 \n",
            "Epoch: 238 | Loss: 0.0033796969801187515 \n",
            "Epoch: 239 | Loss: 0.003331121290102601 \n",
            "Epoch: 240 | Loss: 0.0032832736615091562 \n",
            "Epoch: 241 | Loss: 0.0032360863406211138 \n",
            "Epoch: 242 | Loss: 0.003189569106325507 \n",
            "Epoch: 243 | Loss: 0.0031437212601304054 \n",
            "Epoch: 244 | Loss: 0.00309855118393898 \n",
            "Epoch: 245 | Loss: 0.0030540102161467075 \n",
            "Epoch: 246 | Loss: 0.003010107669979334 \n",
            "Epoch: 247 | Loss: 0.002966855186969042 \n",
            "Epoch: 248 | Loss: 0.002924227388575673 \n",
            "Epoch: 249 | Loss: 0.0028822198510169983 \n",
            "Epoch: 250 | Loss: 0.0028407613281160593 \n",
            "Epoch: 251 | Loss: 0.0027999384328722954 \n",
            "Epoch: 252 | Loss: 0.0027597101870924234 \n",
            "Epoch: 253 | Loss: 0.0027200430631637573 \n",
            "Epoch: 254 | Loss: 0.002680964069440961 \n",
            "Epoch: 255 | Loss: 0.0026424310635775328 \n",
            "Epoch: 256 | Loss: 0.002604437991976738 \n",
            "Epoch: 257 | Loss: 0.0025670381728559732 \n",
            "Epoch: 258 | Loss: 0.00253011635504663 \n",
            "Epoch: 259 | Loss: 0.0024937756825238466 \n",
            "Epoch: 260 | Loss: 0.0024579314049333334 \n",
            "Epoch: 261 | Loss: 0.002422598423436284 \n",
            "Epoch: 262 | Loss: 0.00238778255879879 \n",
            "Epoch: 263 | Loss: 0.002353464253246784 \n",
            "Epoch: 264 | Loss: 0.002319654915481806 \n",
            "Epoch: 265 | Loss: 0.0022863149642944336 \n",
            "Epoch: 266 | Loss: 0.002253437414765358 \n",
            "Epoch: 267 | Loss: 0.0022210748866200447 \n",
            "Epoch: 268 | Loss: 0.0021891393698751926 \n",
            "Epoch: 269 | Loss: 0.0021576862782239914 \n",
            "Epoch: 270 | Loss: 0.002126675331965089 \n",
            "Epoch: 271 | Loss: 0.002096107229590416 \n",
            "Epoch: 272 | Loss: 0.0020659768488258123 \n",
            "Epoch: 273 | Loss: 0.0020362825598567724 \n",
            "Epoch: 274 | Loss: 0.0020070404279977083 \n",
            "Epoch: 275 | Loss: 0.0019781801383942366 \n",
            "Epoch: 276 | Loss: 0.001949760247953236 \n",
            "Epoch: 277 | Loss: 0.0019217318622395396 \n",
            "Epoch: 278 | Loss: 0.0018941342132166028 \n",
            "Epoch: 279 | Loss: 0.0018668919801712036 \n",
            "Epoch: 280 | Loss: 0.0018400464905425906 \n",
            "Epoch: 281 | Loss: 0.001813608454540372 \n",
            "Epoch: 282 | Loss: 0.0017875509802252054 \n",
            "Epoch: 283 | Loss: 0.0017618723213672638 \n",
            "Epoch: 284 | Loss: 0.0017365394160151482 \n",
            "Epoch: 285 | Loss: 0.0017115871887654066 \n",
            "Epoch: 286 | Loss: 0.001686999574303627 \n",
            "Epoch: 287 | Loss: 0.0016627437435090542 \n",
            "Epoch: 288 | Loss: 0.0016388592775911093 \n",
            "Epoch: 289 | Loss: 0.0016153038013726473 \n",
            "Epoch: 290 | Loss: 0.0015920883743092418 \n",
            "Epoch: 291 | Loss: 0.0015692044980823994 \n",
            "Epoch: 292 | Loss: 0.00154665089212358 \n",
            "Epoch: 293 | Loss: 0.0015244126552715898 \n",
            "Epoch: 294 | Loss: 0.0015025067841634154 \n",
            "Epoch: 295 | Loss: 0.001480907085351646 \n",
            "Epoch: 296 | Loss: 0.0014596525579690933 \n",
            "Epoch: 297 | Loss: 0.0014386619441211224 \n",
            "Epoch: 298 | Loss: 0.0014179876307025552 \n",
            "Epoch: 299 | Loss: 0.0013976237969473004 \n",
            "Epoch: 300 | Loss: 0.0013775237603113055 \n",
            "Epoch: 301 | Loss: 0.0013577105710282922 \n",
            "Epoch: 302 | Loss: 0.0013382176402956247 \n",
            "Epoch: 303 | Loss: 0.0013189775636419654 \n",
            "Epoch: 304 | Loss: 0.0013000129256397486 \n",
            "Epoch: 305 | Loss: 0.0012813459616154432 \n",
            "Epoch: 306 | Loss: 0.0012629360426217318 \n",
            "Epoch: 307 | Loss: 0.001244761748239398 \n",
            "Epoch: 308 | Loss: 0.0012268800055608153 \n",
            "Epoch: 309 | Loss: 0.0012092591496184468 \n",
            "Epoch: 310 | Loss: 0.0011918821837753057 \n",
            "Epoch: 311 | Loss: 0.0011747425887733698 \n",
            "Epoch: 312 | Loss: 0.0011578714475035667 \n",
            "Epoch: 313 | Loss: 0.001141220680437982 \n",
            "Epoch: 314 | Loss: 0.0011248134542256594 \n",
            "Epoch: 315 | Loss: 0.0011086390586569905 \n",
            "Epoch: 316 | Loss: 0.0010927258990705013 \n",
            "Epoch: 317 | Loss: 0.0010770251974463463 \n",
            "Epoch: 318 | Loss: 0.001061538583599031 \n",
            "Epoch: 319 | Loss: 0.0010462920181453228 \n",
            "Epoch: 320 | Loss: 0.0010312489466741681 \n",
            "Epoch: 321 | Loss: 0.0010164149571210146 \n",
            "Epoch: 322 | Loss: 0.0010018283501267433 \n",
            "Epoch: 323 | Loss: 0.0009874256793409586 \n",
            "Epoch: 324 | Loss: 0.000973236165009439 \n",
            "Epoch: 325 | Loss: 0.0009592447895556688 \n",
            "Epoch: 326 | Loss: 0.0009454601677134633 \n",
            "Epoch: 327 | Loss: 0.0009318658849224448 \n",
            "Epoch: 328 | Loss: 0.0009184809750877321 \n",
            "Epoch: 329 | Loss: 0.0009052822133526206 \n",
            "Epoch: 330 | Loss: 0.0008922692504711449 \n",
            "Epoch: 331 | Loss: 0.000879440107382834 \n",
            "Epoch: 332 | Loss: 0.0008668083464726806 \n",
            "Epoch: 333 | Loss: 0.0008543392759747803 \n",
            "Epoch: 334 | Loss: 0.0008420652011409402 \n",
            "Epoch: 335 | Loss: 0.000829968717880547 \n",
            "Epoch: 336 | Loss: 0.0008180328877642751 \n",
            "Epoch: 337 | Loss: 0.0008062792476266623 \n",
            "Epoch: 338 | Loss: 0.0007946894038468599 \n",
            "Epoch: 339 | Loss: 0.0007832664996385574 \n",
            "Epoch: 340 | Loss: 0.0007720214780420065 \n",
            "Epoch: 341 | Loss: 0.0007609171443618834 \n",
            "Epoch: 342 | Loss: 0.0007499760831706226 \n",
            "Epoch: 343 | Loss: 0.0007392027182504535 \n",
            "Epoch: 344 | Loss: 0.0007285827305167913 \n",
            "Epoch: 345 | Loss: 0.0007181158871389925 \n",
            "Epoch: 346 | Loss: 0.0007077866466715932 \n",
            "Epoch: 347 | Loss: 0.0006976274307817221 \n",
            "Epoch: 348 | Loss: 0.0006875995895825326 \n",
            "Epoch: 349 | Loss: 0.0006777140079066157 \n",
            "Epoch: 350 | Loss: 0.0006679766229353845 \n",
            "Epoch: 351 | Loss: 0.0006583753856830299 \n",
            "Epoch: 352 | Loss: 0.0006489116931334138 \n",
            "Epoch: 353 | Loss: 0.0006395915406756103 \n",
            "Epoch: 354 | Loss: 0.0006303943227976561 \n",
            "Epoch: 355 | Loss: 0.0006213419837877154 \n",
            "Epoch: 356 | Loss: 0.0006124068750068545 \n",
            "Epoch: 357 | Loss: 0.0006035979022271931 \n",
            "Epoch: 358 | Loss: 0.000594929326325655 \n",
            "Epoch: 359 | Loss: 0.0005863853730261326 \n",
            "Epoch: 360 | Loss: 0.000577960629016161 \n",
            "Epoch: 361 | Loss: 0.0005696425214409828 \n",
            "Epoch: 362 | Loss: 0.0005614495021291077 \n",
            "Epoch: 363 | Loss: 0.0005533965304493904 \n",
            "Epoch: 364 | Loss: 0.0005454288912005723 \n",
            "Epoch: 365 | Loss: 0.0005375945474952459 \n",
            "Epoch: 366 | Loss: 0.0005298770847730339 \n",
            "Epoch: 367 | Loss: 0.0005222632316872478 \n",
            "Epoch: 368 | Loss: 0.000514750718139112 \n",
            "Epoch: 369 | Loss: 0.0005073580541647971 \n",
            "Epoch: 370 | Loss: 0.000500064401421696 \n",
            "Epoch: 371 | Loss: 0.0004928713315166533 \n",
            "Epoch: 372 | Loss: 0.00048579563735984266 \n",
            "Epoch: 373 | Loss: 0.0004788017540704459 \n",
            "Epoch: 374 | Loss: 0.0004719229182228446 \n",
            "Epoch: 375 | Loss: 0.0004651527851819992 \n",
            "Epoch: 376 | Loss: 0.00045846650027669966 \n",
            "Epoch: 377 | Loss: 0.0004518767527770251 \n",
            "Epoch: 378 | Loss: 0.0004453789151739329 \n",
            "Epoch: 379 | Loss: 0.000438972027041018 \n",
            "Epoch: 380 | Loss: 0.0004326683410909027 \n",
            "Epoch: 381 | Loss: 0.00042645708890631795 \n",
            "Epoch: 382 | Loss: 0.0004203241260256618 \n",
            "Epoch: 383 | Loss: 0.00041427818359807134 \n",
            "Epoch: 384 | Loss: 0.0004083382955286652 \n",
            "Epoch: 385 | Loss: 0.0004024588270112872 \n",
            "Epoch: 386 | Loss: 0.00039667641976848245 \n",
            "Epoch: 387 | Loss: 0.00039097724948078394 \n",
            "Epoch: 388 | Loss: 0.00038535241037607193 \n",
            "Epoch: 389 | Loss: 0.0003798272227868438 \n",
            "Epoch: 390 | Loss: 0.0003743621928151697 \n",
            "Epoch: 391 | Loss: 0.0003689836885314435 \n",
            "Epoch: 392 | Loss: 0.0003636784676928073 \n",
            "Epoch: 393 | Loss: 0.00035845788079313934 \n",
            "Epoch: 394 | Loss: 0.00035330350510776043 \n",
            "Epoch: 395 | Loss: 0.0003482167958281934 \n",
            "Epoch: 396 | Loss: 0.00034322409192100167 \n",
            "Epoch: 397 | Loss: 0.00033829198218882084 \n",
            "Epoch: 398 | Loss: 0.00033342637470923364 \n",
            "Epoch: 399 | Loss: 0.0003286298015154898 \n",
            "Epoch: 400 | Loss: 0.00032391081913374364 \n",
            "Epoch: 401 | Loss: 0.00031925406074151397 \n",
            "Epoch: 402 | Loss: 0.0003146650269627571 \n",
            "Epoch: 403 | Loss: 0.00031013478292152286 \n",
            "Epoch: 404 | Loss: 0.0003056821587961167 \n",
            "Epoch: 405 | Loss: 0.00030129001243039966 \n",
            "Epoch: 406 | Loss: 0.0002969626511912793 \n",
            "Epoch: 407 | Loss: 0.0002926944871433079 \n",
            "Epoch: 408 | Loss: 0.0002884809218812734 \n",
            "Epoch: 409 | Loss: 0.00028434887644834816 \n",
            "Epoch: 410 | Loss: 0.0002802602539304644 \n",
            "Epoch: 411 | Loss: 0.00027623024652712047 \n",
            "Epoch: 412 | Loss: 0.00027225917438045144 \n",
            "Epoch: 413 | Loss: 0.00026835111202672124 \n",
            "Epoch: 414 | Loss: 0.00026449403958395123 \n",
            "Epoch: 415 | Loss: 0.0002606920897960663 \n",
            "Epoch: 416 | Loss: 0.00025693723000586033 \n",
            "Epoch: 417 | Loss: 0.00025324663147330284 \n",
            "Epoch: 418 | Loss: 0.00024960661539807916 \n",
            "Epoch: 419 | Loss: 0.0002460175601299852 \n",
            "Epoch: 420 | Loss: 0.00024249251873698086 \n",
            "Epoch: 421 | Loss: 0.00023900732048787177 \n",
            "Epoch: 422 | Loss: 0.00023556608357466757 \n",
            "Epoch: 423 | Loss: 0.0002321852371096611 \n",
            "Epoch: 424 | Loss: 0.00022884551435709 \n",
            "Epoch: 425 | Loss: 0.0002255536091979593 \n",
            "Epoch: 426 | Loss: 0.0002223149931523949 \n",
            "Epoch: 427 | Loss: 0.00021912046940997243 \n",
            "Epoch: 428 | Loss: 0.0002159738214686513 \n",
            "Epoch: 429 | Loss: 0.00021286778792273253 \n",
            "Epoch: 430 | Loss: 0.00020981201669201255 \n",
            "Epoch: 431 | Loss: 0.00020679589943028986 \n",
            "Epoch: 432 | Loss: 0.00020382569346111268 \n",
            "Epoch: 433 | Loss: 0.00020088852033950388 \n",
            "Epoch: 434 | Loss: 0.00019800537847913802 \n",
            "Epoch: 435 | Loss: 0.0001951609447132796 \n",
            "Epoch: 436 | Loss: 0.00019235321087762713 \n",
            "Epoch: 437 | Loss: 0.00018958665896207094 \n",
            "Epoch: 438 | Loss: 0.00018686079420149326 \n",
            "Epoch: 439 | Loss: 0.0001841815101215616 \n",
            "Epoch: 440 | Loss: 0.00018153722339775413 \n",
            "Epoch: 441 | Loss: 0.00017892451432999223 \n",
            "Epoch: 442 | Loss: 0.00017635314725339413 \n",
            "Epoch: 443 | Loss: 0.00017381724319420755 \n",
            "Epoch: 444 | Loss: 0.00017132404900621623 \n",
            "Epoch: 445 | Loss: 0.00016885717923287302 \n",
            "Epoch: 446 | Loss: 0.00016642696573399007 \n",
            "Epoch: 447 | Loss: 0.00016403668269049376 \n",
            "Epoch: 448 | Loss: 0.00016168071306310594 \n",
            "Epoch: 449 | Loss: 0.00015935793635435402 \n",
            "Epoch: 450 | Loss: 0.0001570679887663573 \n",
            "Epoch: 451 | Loss: 0.0001548105210531503 \n",
            "Epoch: 452 | Loss: 0.00015257946506608278 \n",
            "Epoch: 453 | Loss: 0.00015038740821182728 \n",
            "Epoch: 454 | Loss: 0.00014823245874140412 \n",
            "Epoch: 455 | Loss: 0.00014609657227993011 \n",
            "Epoch: 456 | Loss: 0.00014400477812159806 \n",
            "Epoch: 457 | Loss: 0.0001419301552232355 \n",
            "Epoch: 458 | Loss: 0.0001398877939209342 \n",
            "Epoch: 459 | Loss: 0.00013788207434117794 \n",
            "Epoch: 460 | Loss: 0.00013589895388577133 \n",
            "Epoch: 461 | Loss: 0.00013394097913987935 \n",
            "Epoch: 462 | Loss: 0.00013202059199102223 \n",
            "Epoch: 463 | Loss: 0.00013012136332690716 \n",
            "Epoch: 464 | Loss: 0.00012825566227547824 \n",
            "Epoch: 465 | Loss: 0.00012641126522794366 \n",
            "Epoch: 466 | Loss: 0.0001245958119397983 \n",
            "Epoch: 467 | Loss: 0.00012280314695090055 \n",
            "Epoch: 468 | Loss: 0.00012103690824005753 \n",
            "Epoch: 469 | Loss: 0.00011929935135412961 \n",
            "Epoch: 470 | Loss: 0.00011758316395571455 \n",
            "Epoch: 471 | Loss: 0.0001158931918325834 \n",
            "Epoch: 472 | Loss: 0.00011422912939451635 \n",
            "Epoch: 473 | Loss: 0.00011258572340011597 \n",
            "Epoch: 474 | Loss: 0.00011097260721726343 \n",
            "Epoch: 475 | Loss: 0.00010937660408671945 \n",
            "Epoch: 476 | Loss: 0.00010780544835142791 \n",
            "Epoch: 477 | Loss: 0.0001062498486135155 \n",
            "Epoch: 478 | Loss: 0.0001047293480951339 \n",
            "Epoch: 479 | Loss: 0.00010322158777853474 \n",
            "Epoch: 480 | Loss: 0.0001017441027215682 \n",
            "Epoch: 481 | Loss: 0.0001002778735710308 \n",
            "Epoch: 482 | Loss: 9.883844904834405e-05 \n",
            "Epoch: 483 | Loss: 9.741289250086993e-05 \n",
            "Epoch: 484 | Loss: 9.601647616364062e-05 \n",
            "Epoch: 485 | Loss: 9.46380605455488e-05 \n",
            "Epoch: 486 | Loss: 9.3272996309679e-05 \n",
            "Epoch: 487 | Loss: 9.193456207867712e-05 \n",
            "Epoch: 488 | Loss: 9.06157583813183e-05 \n",
            "Epoch: 489 | Loss: 8.93081451067701e-05 \n",
            "Epoch: 490 | Loss: 8.80280276760459e-05 \n",
            "Epoch: 491 | Loss: 8.676040306454524e-05 \n",
            "Epoch: 492 | Loss: 8.551433711545542e-05 \n",
            "Epoch: 493 | Loss: 8.428635192103684e-05 \n",
            "Epoch: 494 | Loss: 8.307308598887175e-05 \n",
            "Epoch: 495 | Loss: 8.188177162082866e-05 \n",
            "Epoch: 496 | Loss: 8.070688636507839e-05 \n",
            "Epoch: 497 | Loss: 7.954361353768036e-05 \n",
            "Epoch: 498 | Loss: 7.839754107408226e-05 \n",
            "Epoch: 499 | Loss: 7.727407501079142e-05 \n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "metadata": {
        "id": "C-waj9axg9Kx",
        "outputId": "db8ccc99-e2fc-4445-df02-6dcca7001cc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training) 4 7.989895343780518\n"
          ]
        }
      ]
    }
  ]
}